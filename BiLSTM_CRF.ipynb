{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json\n",
    "import re, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "def read_data(filepath):\n",
    "    df = pd.DataFrame(columns=['word', 'ne'])  # syntatic chunck, named entity\n",
    "    char = set()\n",
    "    nrow = 0\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        \n",
    "        word, ne = [], []\n",
    "        for row in file:\n",
    "            row = row.strip()\n",
    "            if '-DOCSTART-' in row:\n",
    "                next(file)\n",
    "                continue\n",
    "                \n",
    "            if row:\n",
    "                row = row.split(' ')\n",
    "                word.append(row[0].lower())\n",
    "                ne.append(row[3])\n",
    "            else:\n",
    "                df.loc[nrow] = [word, ne]\n",
    "                word, ne = [], []\n",
    "                nrow += 1\n",
    "    \n",
    "    char2index = {}\n",
    "    chars = set(''.join([w for word in df['word'] for w in word] + ['#']))\n",
    "    for i, char in enumerate(sorted(chars)):\n",
    "        char2index[char] = i\n",
    "    char2index.update({'<unk>': len(char2index), '<pad>': len(char2index) + 1})\n",
    "    return df, char2index\n",
    "\n",
    "\n",
    "def read_wordvec(wordvec_file):\n",
    "    \n",
    "    vocab = set()\n",
    "    wordvec = []\n",
    "    with open(wordvec_file, 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.strip().split()\n",
    "            vocab.add(line[0])\n",
    "            wordvec.append(line[1:])\n",
    "    \n",
    "    vocab.add('<UNK>')\n",
    "    vocab.add('<PAD>')\n",
    "    wordvec.extend([[0] * len(wordvec[0])] * 2)\n",
    "    word2index = {}\n",
    "    index2word = []\n",
    "    for i, word in enumerate(sorted(vocab)):\n",
    "        word2index[word] = i\n",
    "        index2word.append(word)\n",
    "    return np.array(wordvec, dtype=np.float32), word2index, index2word\n",
    "        \n",
    "\n",
    "def normalize_number(words):\n",
    "    return [re.sub(r'[0-9]+[\\+|\\-|,|.|/]?[0-9]+', '0', word) for word in words]\n",
    "\n",
    "\n",
    "def tag_to_index(tags):\n",
    "    tag2index = {'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n",
    "    return [tag2index[tag] for tag in tags]\n",
    "\n",
    "\n",
    "def index_to_tag(tags):\n",
    "    index2tag = ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']    \n",
    "    return [index2tag[tag] for tag in tags]\n",
    "\n",
    "\n",
    "def word_to_index(words, word2index):\n",
    "    unk_word = word2index['<UNK>']\n",
    "    return [word2index.get(word, unk_word) for word in words]\n",
    "\n",
    "\n",
    "def char_to_index(words, char2index):\n",
    "    unk_char = char2index['<unk>']\n",
    "    return [[char2index.get(char, unk_char) for char in word] for word in words]\n",
    "\n",
    "\n",
    "def preprocess(train_df, word2index, char2index):\n",
    "    \n",
    "    train_df['word'] = train_df['word'].map(normalize_number)\n",
    "    xxx = train_df.copy()\n",
    "    train_df['char'] = train_df['word'].map(lambda words: [list(word) for word in words])\n",
    "    train_df['word'] = train_df['word'].map(lambda words: word_to_index(words, word2index))\n",
    "    train_df['char'] = train_df['char'].map(lambda words: char_to_index(words, char2index))\n",
    "    train_df['ne']   = train_df['ne'].map(tag_to_index)    \n",
    "    \n",
    "    word_len = np.array([len(x) for x in train_df['word']])\n",
    "    char_en = np.array([[len(word) for word in words] for words in train_df['char'] ])\n",
    "    \n",
    "    return train_df['word'].values, train_df['char'].values, train_df['ne'].values, word_len, char_en, xxx\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    word_sent = np.load('data/word_sent.npy')\n",
    "    char_sent = np.load('data/char_sent.npy')\n",
    "    tag = np.load('data/tag.npy')\n",
    "    word_len = np.load('data/word_len.npy')\n",
    "    char_len = np.load('data/char_len.npy')\n",
    "    wordvec = np.load('data/wordvec.npy')\n",
    "    \n",
    "    with open('data/word2index.json', 'r', encoding='utf-8') as file:\n",
    "        word2index = json.load(file)\n",
    "        \n",
    "    with open('data/index2word.json', 'r', encoding='utf-8') as file:\n",
    "        index2word = json.load(file)\n",
    "\n",
    "    with open('data/char2index.json', 'r', encoding='utf-8') as file:\n",
    "        char2index = json.load(file)\n",
    "    \n",
    "    return word_sent, char_sent, tag, word_len, char_len, wordvec, word2index, index2word, char2index\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    word_sent = np.load('data/test_word_sent.npy')\n",
    "    char_sent = np.load('data/test_char_sent.npy')\n",
    "    tag = np.load('data/test_tag.npy')\n",
    "    word_len = np.load('data/test_word_len.npy')\n",
    "    char_len = np.load('data/test_char_len.npy')\n",
    "    wordvec = np.load('data/wordvec.npy')\n",
    "    \n",
    "    with open('data/word2index.json', 'r', encoding='utf-8') as file:\n",
    "        word2index = json.load(file)\n",
    "        \n",
    "    with open('data/index2word.json', 'r', encoding='utf-8') as file:\n",
    "        index2word = json.load(file)\n",
    "\n",
    "    with open('data/char2index.json', 'r', encoding='utf-8') as file:\n",
    "        char2index = json.load(file)\n",
    "    \n",
    "    return word_sent, char_sent, tag, word_len, char_len, wordvec, word2index, index2word, char2index\n",
    "\n",
    "\n",
    "def train_val_split(x_train, x_char_train, y_train, seq_len, word_len, train_ratio=.7):\n",
    "    train_len  = int(len(x_train) * train_ratio)\n",
    "    train_data = [x_train[: train_len], x_char_train[: train_len], y_train[: train_len], seq_len[: train_len], word_len[: train_len]]\n",
    "    val_data   = [x_train[train_len: ], x_char_train[train_len: ], y_train[train_len: ], seq_len[train_len: ], word_len[train_len: ]] \n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def shuffle_data(data):\n",
    "    indice = np.arange(len(data[0]))\n",
    "    np.random.shuffle(indice)\n",
    "    \n",
    "    return [d[indice] for d in data]\n",
    "\n",
    "\n",
    "def next_batch(data, batch_size, word2index, char2index):\n",
    "    def pad(sequence, max_wlen, pad_token):\n",
    "        return np.array([seq + [pad_token] * (max_wlen - len(seq)) for seq in sequence])\n",
    "    \n",
    "    def char_pad(sequence, max_wlen, max_clen, pad_token):\n",
    "        pad_seq = []\n",
    "        for words in sequence:\n",
    "            pad_words = words + [[pad_token]] * (max_wlen - len(words))            \n",
    "            pad_seq.append([word + [pad_token] * (max_clen - len(word)) for word in pad_words])\n",
    "        return np.array(pad_seq)\n",
    "            \n",
    "\n",
    "    word_sent, char_sent, tag, word_len, char_len = data[0], data[1], data[2], data[3], data[4] \n",
    "    n_batch = len(word_sent) // batch_size\n",
    "    for i in range(n_batch):\n",
    "        offset = i * batch_size\n",
    "        indice = np.arange(offset, offset + batch_size)\n",
    "        batch_wlen = word_len[indice]\n",
    "        batch_clen = pad(char_len[indice], max(batch_wlen), 0)\n",
    "        batch_word = pad(word_sent[indice], max(batch_wlen), word2index['<PAD>'])\n",
    "        batch_char = char_pad(char_sent[indice], max(batch_wlen), max([max(clen) for clen in batch_clen]), char2index['<pad>'])\n",
    "        batch_tag  = pad(tag[indice], max(batch_wlen), 8) if tag.any() else []\n",
    "        \n",
    "        yield batch_word, batch_char, batch_tag, batch_wlen, batch_clen\n",
    "    \n",
    "    \n",
    "    \n",
    "    offset = n_batch * batch_size\n",
    "    if offset == len(word_sent):\n",
    "        return\n",
    "    \n",
    "    batch_wlen = word_len[offset: ]\n",
    "    batch_clen = pad(char_len[offset: ], max(batch_wlen), 0)\n",
    "    batch_word = pad(word_sent[offset: ], max(batch_wlen), word2index['<PAD>'])\n",
    "    batch_char = char_pad(char_sent[offset: ], max(batch_wlen), max([max(clen) for clen in batch_clen]), char2index['<pad>'])\n",
    "    batch_tag  = pad(tag[offset: ], max(batch_wlen), 8) if tag.any() else []\n",
    "\n",
    "    yield batch_word, batch_char, batch_tag, batch_wlen, batch_clen\n",
    "\n",
    "\n",
    "def get_entities(sequence_tag):\n",
    "    \n",
    "#     entity = {\n",
    "#         'begin': 0,\n",
    "#         'end': 0,\n",
    "#         'type': '' \n",
    "#     }\n",
    "    entities = []\n",
    "    is_ne = False\n",
    "    ne_type = ''\n",
    "    for i, tag in enumerate(sequence_tag):\n",
    "        if is_ne and ('B-' in tag or 'O' in tag):\n",
    "            entities.append({'begin': begin, 'end': i, 'type': ne_type}) \n",
    "            is_ne = False\n",
    "        if 'B-' in tag:\n",
    "            begin = i\n",
    "            is_ne = True\n",
    "            ne_type = tag.split('-')[1]\n",
    "        elif 'I-' in tag and ne_type != tag.split('-')[1]:\n",
    "            is_ne = False\n",
    "    if is_ne:\n",
    "        entities.append({'begin': begin, 'end': i, 'type': ne_type})\n",
    "        \n",
    "    return entities     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.py\n",
    "\n",
    "def accuracy(predictions, tags):\n",
    "    def equal(pred_ne, tag_ne):\n",
    "        return pred_ne\n",
    "    total, correct = 0, 0\n",
    "    for prediction, tag in zip(predictions, tags):\n",
    "        pred_entities = get_entities(prediction)\n",
    "        tag_entities =  get_entities(tag)\n",
    "        t_len, p_len, offset = len(tag_entities), len(pred_entities), 0\n",
    "        for tag_ne in tag_entities:\n",
    "            for i in range(offset, p_len):\n",
    "                if tag_ne == pred_entities[i]:\n",
    "                    correct += 1\n",
    "                    offset += 1\n",
    "        total += t_len\n",
    "        \n",
    "    return correct / total, total, correct\n",
    "                                        \n",
    "\n",
    "   \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 860 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wordvec_file = 'wordvectors/glove.6B.100d.txt'\n",
    "wordvec, word2index, index2word = read_wordvec(wordvec_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/train.txt'\n",
    "test_file = 'data/test.txt'\n",
    "data_df, char2index = read_data(data_file)\n",
    "test_df, _ = read_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[eu, rejects, german, call, to, boycott, briti...</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[peter, blackburn]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brussels, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the, european, commission, said, on, thursday...</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[germany, 's, representative, to, the, europea...</td>\n",
       "      <td>[B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                word  \\\n",
       "0  [eu, rejects, german, call, to, boycott, briti...   \n",
       "1                                 [peter, blackburn]   \n",
       "2                             [brussels, 1996-08-22]   \n",
       "3  [the, european, commission, said, on, thursday...   \n",
       "4  [germany, 's, representative, to, the, europea...   \n",
       "\n",
       "                                                  ne  \n",
       "0          [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]  \n",
       "1                                     [B-PER, I-PER]  \n",
       "2                                         [B-LOC, O]  \n",
       "3  [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...  \n",
       "4  [B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent, char_sent, tag, word_len, char_len, data = preprocess(data_df.copy(), word2index, char2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[eu, rejects, german, call, to, boycott, briti...</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[peter, blackburn]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brussels, 0]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the, european, commission, said, on, thursday...</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[germany, 's, representative, to, the, europea...</td>\n",
       "      <td>[B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\", we, do, n't, support, any, such, recommend...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[he, said, further, scientific, study, was, re...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[he, said, a, proposal, last, month, by, eu, f...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-ORG, O, O, B-PER, I-PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[fischler, proposed, eu0wide, measures, after,...</td>\n",
       "      <td>[B-PER, O, B-MISC, O, O, O, O, B-LOC, O, B-LOC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[but, fischler, agreed, to, review, his, propo...</td>\n",
       "      <td>[O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[spanish, farm, minister, loyola, de, palacio,...</td>\n",
       "      <td>[B-MISC, O, O, B-PER, I-PER, I-PER, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[only, france, and, britain, backed, fischler,...</td>\n",
       "      <td>[O, B-LOC, O, B-LOC, O, B-PER, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[the, eu, 's, scientific, veterinary, and, mul...</td>\n",
       "      <td>[O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[sheep, have, long, been, known, to, contract,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[british, farmers, denied, on, thursday, there...</td>\n",
       "      <td>[B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[\", what, we, have, to, be, extremely, careful...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[bonn, has, led, efforts, to, protect, public,...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[germany, imported, 0, sheep, from, britain, l...</td>\n",
       "      <td>[B-LOC, O, O, O, O, B-LOC, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[it, brought, in, 0, tonnes, of, british, mutt...</td>\n",
       "      <td>[O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[rare, hendrix, song, draft, sells, for, almos...</td>\n",
       "      <td>[O, B-PER, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[london, 0]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[a, rare, early, handwritten, draft, of, a, so...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-LOC, O, O, B-PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[a, florida, restaurant, paid, 0, pounds, (, $...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[at, the, end, of, a, january, 0, concert, in,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[buyers, also, snapped, up, 0, other, items, t...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[they, included, a, black, lacquer, and, mothe...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[the, guitarist, died, of, a, drugs, overdose,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[china, says, taiwan, spoils, atmosphere, for,...</td>\n",
       "      <td>[B-LOC, O, B-LOC, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[beijing, 0]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>[gov, law, german, home, ctry, =, tax, provs, ...</td>\n",
       "      <td>[O, O, B-MISC, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>[mgt, /, und, 0, sell, conc, 0, praecip, =]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>[underlying, govt, bond, 0, pct, sept, 0]</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>[notes, bayerische, vereinsbank, is, joint, le...</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>[0, london, newsroom, 0, 0, 0, 0]</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[venantius, sets, $, 0, million, january, 0, f...</td>\n",
       "      <td>[B-ORG, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>[london, 0]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>[the, following, floating0rate, issue, was, an...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-OR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>[borrower, venantius, ab, (, swedish, national...</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, B-MISC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>[amt, $, 0, mln, spread, 0, 0, bp, maturity, 0...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[type, frn, base, 0m, libor, pay, date, s0sep0]</td>\n",
       "      <td>[O, O, O, B-ORG, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>[last, moody, aa0, iss, price, 0, full, fees, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>[last, s&amp;p, aa0, reoffer, =]</td>\n",
       "      <td>[O, B-ORG, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>[notes, s, short, first, coupon]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>[listing, london, denoms, (, k, ), 0, sale, li...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, B-LOC, O, B-LO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>[neg, plg, yes, crs, deflt, no, force, maj, ip...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>[gov, law, english, home, ctry, sweden, tax, p...</td>\n",
       "      <td>[O, O, B-MISC, O, O, B-LOC, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>[mgt, /, und, 0, bp, sell, conc, 0, bp, praeci...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>[notes, issued, off, emtn, programme]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>[0, london, newsroom, 0, 0, 0, 0]</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>[port, conditions, update, 0, syria, 0, lloyds...</td>\n",
       "      <td>[O, O, O, O, B-LOC, O, B-ORG, I-ORG, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>[port, conditions, from, lloyds, shipping, int...</td>\n",
       "      <td>[O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>[lattakia, 0, aug, 0, 0, waiting, time, at, la...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, O, B-LOC, O, B-LOC, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[israel, plays, down, fears, of, war, with, sy...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>[colleen, siegel]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[jerusalem, 0]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[israel, 's, outgoing, peace, negotiator, with...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-LOC, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[itamar, rabinovich, 0, who, as, israel, 's, a...</td>\n",
       "      <td>[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[\", it, appears, to, me, the, syrian, priority...</td>\n",
       "      <td>[O, O, O, O, O, O, B-MISC, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[the, syrians, are, confused, 0, they, are, de...</td>\n",
       "      <td>[O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 word  \\\n",
       "0   [eu, rejects, german, call, to, boycott, briti...   \n",
       "1                                  [peter, blackburn]   \n",
       "2                                       [brussels, 0]   \n",
       "3   [the, european, commission, said, on, thursday...   \n",
       "4   [germany, 's, representative, to, the, europea...   \n",
       "5   [\", we, do, n't, support, any, such, recommend...   \n",
       "6   [he, said, further, scientific, study, was, re...   \n",
       "7   [he, said, a, proposal, last, month, by, eu, f...   \n",
       "8   [fischler, proposed, eu0wide, measures, after,...   \n",
       "9   [but, fischler, agreed, to, review, his, propo...   \n",
       "10  [spanish, farm, minister, loyola, de, palacio,...   \n",
       "11                                                [0]   \n",
       "12  [only, france, and, britain, backed, fischler,...   \n",
       "13  [the, eu, 's, scientific, veterinary, and, mul...   \n",
       "14  [sheep, have, long, been, known, to, contract,...   \n",
       "15  [british, farmers, denied, on, thursday, there...   \n",
       "16  [\", what, we, have, to, be, extremely, careful...   \n",
       "17  [bonn, has, led, efforts, to, protect, public,...   \n",
       "18  [germany, imported, 0, sheep, from, britain, l...   \n",
       "19  [it, brought, in, 0, tonnes, of, british, mutt...   \n",
       "20  [rare, hendrix, song, draft, sells, for, almos...   \n",
       "21                                        [london, 0]   \n",
       "22  [a, rare, early, handwritten, draft, of, a, so...   \n",
       "23  [a, florida, restaurant, paid, 0, pounds, (, $...   \n",
       "24  [at, the, end, of, a, january, 0, concert, in,...   \n",
       "25  [buyers, also, snapped, up, 0, other, items, t...   \n",
       "26  [they, included, a, black, lacquer, and, mothe...   \n",
       "27  [the, guitarist, died, of, a, drugs, overdose,...   \n",
       "28  [china, says, taiwan, spoils, atmosphere, for,...   \n",
       "29                                       [beijing, 0]   \n",
       "..                                                ...   \n",
       "70  [gov, law, german, home, ctry, =, tax, provs, ...   \n",
       "71        [mgt, /, und, 0, sell, conc, 0, praecip, =]   \n",
       "72          [underlying, govt, bond, 0, pct, sept, 0]   \n",
       "73  [notes, bayerische, vereinsbank, is, joint, le...   \n",
       "74                  [0, london, newsroom, 0, 0, 0, 0]   \n",
       "75  [venantius, sets, $, 0, million, january, 0, f...   \n",
       "76                                        [london, 0]   \n",
       "77  [the, following, floating0rate, issue, was, an...   \n",
       "78  [borrower, venantius, ab, (, swedish, national...   \n",
       "79  [amt, $, 0, mln, spread, 0, 0, bp, maturity, 0...   \n",
       "80    [type, frn, base, 0m, libor, pay, date, s0sep0]   \n",
       "81  [last, moody, aa0, iss, price, 0, full, fees, ...   \n",
       "82                       [last, s&p, aa0, reoffer, =]   \n",
       "83                   [notes, s, short, first, coupon]   \n",
       "84  [listing, london, denoms, (, k, ), 0, sale, li...   \n",
       "85  [neg, plg, yes, crs, deflt, no, force, maj, ip...   \n",
       "86  [gov, law, english, home, ctry, sweden, tax, p...   \n",
       "87  [mgt, /, und, 0, bp, sell, conc, 0, bp, praeci...   \n",
       "88              [notes, issued, off, emtn, programme]   \n",
       "89                  [0, london, newsroom, 0, 0, 0, 0]   \n",
       "90  [port, conditions, update, 0, syria, 0, lloyds...   \n",
       "91  [port, conditions, from, lloyds, shipping, int...   \n",
       "92  [lattakia, 0, aug, 0, 0, waiting, time, at, la...   \n",
       "93  [israel, plays, down, fears, of, war, with, sy...   \n",
       "94                                  [colleen, siegel]   \n",
       "95                                     [jerusalem, 0]   \n",
       "96  [israel, 's, outgoing, peace, negotiator, with...   \n",
       "97  [itamar, rabinovich, 0, who, as, israel, 's, a...   \n",
       "98  [\", it, appears, to, me, the, syrian, priority...   \n",
       "99  [the, syrians, are, confused, 0, they, are, de...   \n",
       "\n",
       "                                                   ne  \n",
       "0           [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]  \n",
       "1                                      [B-PER, I-PER]  \n",
       "2                                          [B-LOC, O]  \n",
       "3   [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...  \n",
       "4   [B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...  \n",
       "5   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "6   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "7   [O, O, O, O, O, O, O, B-ORG, O, O, B-PER, I-PE...  \n",
       "8   [B-PER, O, B-MISC, O, O, O, O, B-LOC, O, B-LOC...  \n",
       "9   [O, B-PER, O, O, O, O, O, O, O, B-ORG, O, O, O...  \n",
       "10  [B-MISC, O, O, B-PER, I-PER, I-PER, O, O, O, B...  \n",
       "11                                                [O]  \n",
       "12            [O, B-LOC, O, B-LOC, O, B-PER, O, O, O]  \n",
       "13  [O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "14  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-M...  \n",
       "15  [B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "16  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "17  [B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "18  [B-LOC, O, O, O, O, B-LOC, O, O, O, O, O, O, O...  \n",
       "19  [O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O...  \n",
       "20                 [O, B-PER, O, O, O, O, O, O, O, O]  \n",
       "21                                         [B-LOC, O]  \n",
       "22  [O, O, O, O, O, O, O, O, O, B-LOC, O, O, B-PER...  \n",
       "23  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "24  [O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, B...  \n",
       "25  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-P...  \n",
       "26  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...  \n",
       "27               [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "28                   [B-LOC, O, B-LOC, O, O, O, O, O]  \n",
       "29                                         [B-LOC, O]  \n",
       "..                                                ...  \n",
       "70                   [O, O, B-MISC, O, O, O, O, O, O]  \n",
       "71                        [O, O, O, O, O, O, O, O, O]  \n",
       "72                              [O, O, O, O, O, O, O]  \n",
       "73                      [O, B-ORG, I-ORG, O, O, O, O]  \n",
       "74                      [O, B-ORG, I-ORG, O, O, O, O]  \n",
       "75                    [B-ORG, O, O, O, O, O, O, O, O]  \n",
       "76                                         [B-LOC, O]  \n",
       "77  [O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-OR...  \n",
       "78           [O, B-ORG, I-ORG, O, B-MISC, O, O, O, O]  \n",
       "79                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "80                       [O, O, O, B-ORG, O, O, O, O]  \n",
       "81                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "82                                [O, B-ORG, O, O, O]  \n",
       "83                                    [O, O, O, O, O]  \n",
       "84  [O, B-LOC, O, O, O, O, O, O, O, B-LOC, O, B-LO...  \n",
       "85                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "86               [O, O, B-MISC, O, O, B-LOC, O, O, O]  \n",
       "87                  [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "88                                    [O, O, O, O, O]  \n",
       "89                      [O, B-ORG, I-ORG, O, O, O, O]  \n",
       "90            [O, O, O, O, B-LOC, O, B-ORG, I-ORG, O]  \n",
       "91           [O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O]  \n",
       "92  [B-LOC, O, O, O, O, O, O, O, B-LOC, O, B-LOC, ...  \n",
       "93                [B-LOC, O, O, O, O, O, O, B-LOC, O]  \n",
       "94                                     [B-PER, I-PER]  \n",
       "95                                         [B-LOC, O]  \n",
       "96  [B-LOC, O, O, O, O, O, B-LOC, O, O, O, O, O, O...  \n",
       "97  [B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC,...  \n",
       "98       [O, O, O, O, O, O, B-MISC, O, O, O, O, O, O]  \n",
       "99  [O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/word_sent.npy', word_sent)\n",
    "np.save('data/char_sent.npy', char_sent)\n",
    "np.save('data/tag.npy', tag)\n",
    "np.save('data/word_len.npy', word_len)\n",
    "np.save('data/char_len.npy', char_len)\n",
    "np.save('data/wordvec.npy', wordvec)\n",
    "\n",
    "with open('data/word2index.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(word2index, file)\n",
    "    \n",
    "with open('data/index2word.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(index2word, file)\n",
    "    \n",
    "with open('data/char2index.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(char2index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_sent, test_char_sent, test_tag, test_word_len, test_char_len = preprocess(test_df.copy(), word2index, char2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/test_word_sent.npy', test_word_sent)\n",
    "np.save('data/test_char_sent.npy', test_char_sent)\n",
    "np.save('data/test_tag.npy', test_tag)\n",
    "np.save('data/test_word_len.npy', test_word_len)\n",
    "np.save('data/test_char_len.npy', test_char_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.nn import embedding_lookup, bidirectional_dynamic_rnn, dropout\n",
    "from tensorflow.nn.rnn_cell import LSTMCell, DropoutWrapper\n",
    "from tensorflow.contrib.crf import crf_log_likelihood, viterbi_decode\n",
    "from tqdm import tqdm_notebook\n",
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.py\n",
    "def BiLSTM(sequence, seq_len, unit, name):\n",
    "    cell_fw = LSTMCell(unit, name='fw' + name)\n",
    "    cell_bw = LSTMCell(unit, name='bw' + name)\n",
    "#     cell_fw = DropoutWrapper(cell_fw,  output_keep_prob=0.5)\n",
    "#     cell_bw = DropoutWrapper(cell_bw,  output_keep_prob=0.5)\n",
    "    \n",
    "    (outputs, states) = bidirectional_dynamic_rnn(cell_fw, cell_bw, sequence, seq_len, dtype=tf.float32)\n",
    "    # outputs: (output_fw, output_bw), both with shape (batch_size, max_len, unit)\n",
    "    # states:  ((cell_fw, state_fw), (cell_bw, state_bw)), fw & bw final state with shape (batch_size, unit)\n",
    "    return outputs, states\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTagger:\n",
    "    def __init__(self, wordvec, config):\n",
    "        '''\n",
    "        ##### placeholder #####\n",
    "        word_sent: word-level sentence, (batch_size, max_word_len)\n",
    "        char_sent: char-level sentence, (batch_size, max_word_len, max_char_len)\n",
    "        word_len:  sentence length, (batch_size, )\n",
    "        char_len:  word length, (batch_size, max_word_len)\n",
    "        tag: answer tag, (batch_size, max_word_len)\n",
    "        '''\n",
    "        tf.reset_default_graph()\n",
    "        self.config = config\n",
    "        \n",
    "        self.hidden_unit = config['hidden_unit']\n",
    "        self.context_lstm_unit = config['context_lstm_unit']\n",
    "        self.char_vocab_size = config['char_vocab_size']\n",
    "        self.char_emb_dim = config['char_emb_dim']\n",
    "        self.char_lstm_unit = config['char_lstm_unit']\n",
    "        self.num_class = config['num_class']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        \n",
    "        self.word_sent = tf.placeholder(tf.int32, (None, None)) \n",
    "        self.char_sent = tf.placeholder(tf.int32, (None, None, None))\n",
    "        self.word_len  = tf.placeholder(tf.int32, (None, ))\n",
    "        self.char_len  = tf.placeholder(tf.int32, (None, None))\n",
    "        self.tag = tf.placeholder(tf.int32, (None, None))\n",
    "        \n",
    "        \n",
    "        self.w = tf.get_variable('hidden_weight', (self.context_lstm_unit * 2, self.hidden_unit)) \n",
    "        self.b = tf.get_variable('hidden_bias', (self.hidden_unit, ))\n",
    "        \n",
    "        self.word_embedding = tf.get_variable(name='word_embedding', \n",
    "                                              shape=wordvec.shape, \n",
    "                                              initializer=tf.constant_initializer(wordvec, dtype=tf.float32),\n",
    "                                              dtype=tf.float32,\n",
    "                                              trainable=config['word_emb_trainable'])\n",
    "        \n",
    "        \n",
    "        self.char_embedding = tf.get_variable(name='char_embedding', \n",
    "                                              shape=(self.char_vocab_size, self.char_emb_dim), \n",
    "                                              initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0),\n",
    "                                              dtype=tf.float32,\n",
    "                                              trainable=True)\n",
    "        \n",
    "    def embedding_layer(self, word_sent, char_sent, char_len):\n",
    "        '''\n",
    "        word_sent: word-level sentence, (batch_size, max_word_len)\n",
    "        char_sent: char-level sentence, (batch_size, max_word_len, max_char_len)\n",
    "        word_len:  sentence length, (batch_size, )\n",
    "        char_len:  word length, (batch_size, max_word_len)\n",
    "        word_embedding: word embedding matrix, (word_vocab_size, word_emb_dim)\n",
    "        '''\n",
    "        max_word_len = tf.shape(word_sent)[1]\n",
    "        max_char_len = tf.shape(char_sent)[2]\n",
    "        \n",
    "        word_emb = embedding_lookup(self.word_embedding, word_sent)  # (batch_size, max_word_len, word_emb_dim)\n",
    "        char_emb = embedding_lookup(self.char_embedding, char_sent)  # (batch_size, max_word_len, max_char_len, char_emb_dim)\n",
    "\n",
    "        \n",
    "        # Reshape char_emb to 3D tensor for BiLSTM input\n",
    "        char_emb = tf.reshape(char_emb, (-1, max_char_len, self.char_emb_dim))  # (batch_size * max_word_len, max_char_len, char_emb_dim)\n",
    "        char_len = tf.reshape(char_len, (-1, ))   # (batch_size * max_word_len, )\n",
    "        \n",
    "        # Get final states which represent char-level representation\n",
    "        _, states = BiLSTM(char_emb, char_len, self.char_lstm_unit, 'char_embedding')\n",
    "        final_h = [states[0][1], states[1][1]]  # [forward final state, backward final state]\n",
    "        char_emb = tf.concat(final_h, axis=1)  # (batch_size * max_word_len, char_lstm_unit * 2)\n",
    "        \n",
    "        # Reshape char_emb to match word_emb's shape for concatenating both tensors\n",
    "        char_emb = tf.reshape(char_emb, (-1, max_word_len, self.char_lstm_unit * 2))  # (batch_size, max_word_len, char_lstm_unit * 2)\n",
    "        \n",
    "        # Token Representation\n",
    "        emb = tf.concat([word_emb, char_emb], axis=2)  # (batch_size, max_word_len, word_emb_dim + char_lstm_unit * 2)\n",
    "        emb = dropout(emb, keep_prob=0.5)\n",
    "        return emb\n",
    "    \n",
    "    \n",
    "    def build(self):\n",
    "        ##### Context Representation #####\n",
    "        # emb_dim = word_emb_dim + char_lstm_unit * 2\n",
    "        word_rep   = self.embedding_layer(self.word_sent, self.char_sent, self.char_len)   # (batch_size, max_word_len, emb_dim)\n",
    "        outputs, _ = BiLSTM(word_rep, self.word_len, self.context_lstm_unit, 'context_representation') \n",
    "        context    = tf.concat(outputs, axis=2)  # (batch_size, max_word_len, context_lstm_unit * 2)\n",
    "        \n",
    "        ##### Hidden Layer #####\n",
    "        max_word_len = tf.shape(context)[1]\n",
    "        context      = tf.reshape(context, (-1, self.context_lstm_unit * 2))   # (batch_size * max_word_len, context_lstm_unit * 2)\n",
    "        dense        = tf.matmul(context, self.w) + self.b                     # (batch_size * max_word_len, hidden_unit)\n",
    "        self.scores  = tf.reshape(dense, (-1, max_word_len, 100))   # (batch_size, max_word_len, hidden_unit)\n",
    "        \n",
    "        ##### CRF #####\n",
    "        log_likelihood, self.transition_params = crf_log_likelihood(self.scores, self.tag, self.word_len)\n",
    "        self.loss = tf.reduce_mean(-log_likelihood)\n",
    "        \n",
    "#         self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "                \n",
    "    \n",
    "    def fit(self, train_data, val_data, epoch_size, batch_size, word2index, model_name):\n",
    "        def learn(data, epoch, mode):\n",
    "            tn = tqdm_notebook(total=len(data[0]))\n",
    "            nbatch, epoch_loss, epoch_acc = 0, 0, 0 \n",
    "            for batch_word, batch_char, batch_tag, batch_wlen, batch_clen in next_batch(data, batch_size, word2index, char2index):\n",
    "                feed_dict = {\n",
    "                    self.word_sent: batch_word,\n",
    "                    self.char_sent: batch_char, \n",
    "                    self.word_len: batch_wlen,\n",
    "                    self.char_len: batch_clen,\n",
    "                    self.tag: batch_tag\n",
    "                }\n",
    "                if mode == 'train':\n",
    "                    fetches = [self.loss, self.optimizer]\n",
    "                    loss, _ = self.sess.run(fetches, feed_dict)\n",
    "                    tn.set_description('Epoch: {}/{}'.format(epoch, epoch_size))\n",
    "                elif mode == 'validate':                    \n",
    "                    fetches = [self.loss]\n",
    "                    loss = self.sess.run(fetches, feed_dict)[0]\n",
    "                \n",
    "#                 acc = accuracy(output, label)\n",
    "                tn.set_postfix(loss=loss, mode=mode)\n",
    "                tn.update(n=len(batch_word))\n",
    "                \n",
    "                epoch_loss += loss\n",
    "#                 epoch_acc += acc\n",
    "                nbatch += 1\n",
    "            \n",
    "            tn.set_postfix(loss=epoch_loss/nbatch, mode=mode)\n",
    "            return [epoch_loss/nbatch]\n",
    "                \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "        train_log, val_log = [], []\n",
    "        print('Train on {} samples, validate on {} samples'.format(len(train_data[0]), len(val_data[0])))\n",
    "        for epoch in range(1, epoch_size + 1):       \n",
    "            train_data = shuffle_data(train_data)\n",
    "            # train\n",
    "            train_log.append(learn(train_data, epoch, 'train'))\n",
    "\n",
    "            # validate\n",
    "            if len(val_data[0]) > 0:\n",
    "                val_log.append(learn(val_data, epoch, 'validate')) \n",
    "        \n",
    "        self.save(model_name, train_log, val_log)\n",
    "    \n",
    "    \n",
    "    def predict(self, data, word_to_index, char2index):\n",
    "        \n",
    "        tn = tqdm_notebook(total=len(data[0]))\n",
    "        batch_size = 100\n",
    "        transition_params = self.transition_params.eval(session=self.sess)\n",
    "        prediction = []\n",
    "        for batch_word, batch_char, batch_tag, batch_wlen, batch_clen in next_batch(data, batch_size, word2index, char2index):\n",
    "            fetches = [self.scores]\n",
    "            feed_dict = {\n",
    "                self.word_sent: batch_word,\n",
    "                self.char_sent: batch_char, \n",
    "                self.word_len: batch_wlen,\n",
    "                self.char_len: batch_clen\n",
    "            }\n",
    "            scores = self.sess.run(fetches, feed_dict)[0]\n",
    "            scores = [score[:wlen] for score, wlen in zip(scores, batch_wlen)]\n",
    "            prediction.extend([viterbi_decode(score, transition_params)[0] for score in scores])\n",
    "            \n",
    "            tn.set_postfix(mode='predict')\n",
    "            tn.update(n=len(batch_word))\n",
    "        \n",
    "        return np.array(prediction)\n",
    "    \n",
    "    \n",
    "    def save(self, model_name, train_log, val_log):\n",
    "        model_dir = 'models/{}'.format(model_name)\n",
    "        if not os.path.isdir(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "            os.mkdir('{}/result'.format(model_dir))\n",
    "        \n",
    "        # save model\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.sess, '{}/{}.ckpt'.format(model_dir, model_name))\n",
    "        \n",
    "        # save config\n",
    "        with open('{}/config.json'.format(model_dir), 'w', encoding='utf-8') as file:\n",
    "            json.dump(self.config, file)\n",
    "            \n",
    "        # save log\n",
    "        with open('{}/log'.format(model_dir), 'w', encoding='utf-8') as file:\n",
    "            for i in range(len(train_log)):\n",
    "                tlog = train_log[i]\n",
    "                vlog = val_log[i] if len(val_log) > 0 else []\n",
    "                log_str = 'Epoch {}: train_loss={}'.format(i+1, tlog[0])\n",
    "                log_str += ', val_loss={}'.format(vlog[0]) if vlog else ''\n",
    "                file.write(log_str + '\\n')\n",
    "            \n",
    "        print('Model was saved in {}'.format(save_path))\n",
    "    \n",
    "    \n",
    "    def restore(self, model_path):\n",
    "        saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        saver.restore(self.sess, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sent, char_sent, tag, word_len, char_len, wordvec, word2index, index2word, char2index = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_val_split(word_sent, char_sent, tag, word_len, char_len, train_ratio=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch_size = 30\n",
    "batch_size = 1\n",
    "config = {\n",
    "    'num_class': 9,\n",
    "    'char_lstm_unit': 25,\n",
    "    'context_lstm_unit': 100,\n",
    "    'hidden_unit': 100,\n",
    "    'word_emb_dim': len(wordvec[0]),\n",
    "    'char_emb_dim': 25,\n",
    "    'word_vocab_size': len(wordvec),\n",
    "    'char_vocab_size': len(char2index),\n",
    "    'learning_rate': 1e-2,\n",
    "    'wordvec': wordvec_file,\n",
    "    'word_emb_trainable': True,\n",
    "    'epoch_size': epoch_size,\n",
    "    'batch_size': batch_size\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = NERTagger(wordvec, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagger.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-18\n",
      "Train on 13832 samples, validate on 3459 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92efda87de924bd29271ebecffef1c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59d343b7741476ebd5ab771b58b9780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd830bad61d40b296b501d4e73c9394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dac5df21a64ba58ad9d09ae2639680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9670a119bd45999e5b189b71017c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8ff36f2a41425b82b13017136ba583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f94a2e6eb1c4eacbef9496980960505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d076eb1daf049bab30174721c1d508c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce705c86dca94d43ba82c55f58a3bbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88eb99954534b7aae52e7d94e0d6acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e720e8f5fd4fe28286d855c33a69e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b917b13f647d4c6da4075bb9b9af1ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1367721d1f549edb9f7d23c09af8a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68262aa631a4fde9f8c46d5c7ccecf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0efc0b822a748879d455abea2dfb4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88e05b1bb414effb75400f19d7cfd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e4f892526041efaa60e69c0ae37166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dccb7fa34d48a2891014d57e677733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b11e832a194db09f1b5e68cd32b26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7514f6a9384673a4a2f4f2f576e42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1480e1f50b45858a7e35047441cee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6622a3db32124dc2922a31970a047c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b0ce2ee93a4750baa1a2baa50b600a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab25096862574601a93c495e95ffeb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d09827fa96418496618d2da28d1644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64dc3a0f1314a69b458a868e0c0a3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb1b412af874eb19b4c3b39fc2aa966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c851d70b5b354d3ca70fdc2472b4d395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8346041b880845419b1673937fa6dfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3636b7dc9a7402088c2099269b546f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589cf6a3f77b4f5b84cb54c660db226b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bc899f010b4421baecfaed1c1041c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9f4c6f83bf406794de0ce06c47c8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e885ba3dfb1468281aab0d9248a1acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0839a9f79b9942b9b5047f21bef0eb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002b17953c3f4097bc9fa5dbf44a0ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c0a7b06c14489da1684954e69db810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4711ff8080b245dca662112e3838c4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039d8f3c248a4131bc924372d0b3b9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b4484bde1441e89342fc6935b615a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55084527e7904a0dac48e26394409ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17ced0275754a88a5207b98bc6c9bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a393ada808942dba4dcb4e5da17e944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19935da8819c43cc8714c76310e533f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9c8b2332df484297d2b9a4bc10bc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bb742e53da4831b7e4177774e1eaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b16a761533c4e7da2eaf158e8eb5b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97ed2f2ce934a61924f4201bde80dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29f65edeeca4ca7921dee7b7c91760f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e52364599764c18a77c116320f45d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db6af099d2148e8a7c8362eb3ddc072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f851bde6e64c5292e8c3849c8cf083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39fe33e1b1a4acb86ca1352f97ccc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6943b5b647734844806afbede37cf27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d76241d52a496890ef6b2ec3b43cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05890c6290b439ea6674d479546e75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a06aa740594d5eaca04b6158c9e2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14e4d7885a041f08c0df579ac2ef0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cad172fa564e5db5a6d928a700bed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8578b09ec04827a486806663a94344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was saved in models/model-18/model-18.ckpt\n"
     ]
    }
   ],
   "source": [
    "n += 1\n",
    "model_name = 'model-{}'.format(n)\n",
    "print(model_name)\n",
    "tagger.fit(train_data, val_data, epoch_size, batch_size, word2index, model_name)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_sent, test_char_sent, test_tag, test_word_len, test_char_len, _, _, _, _ = load_test_data()\n",
    "test_data = [test_word_sent, test_char_sent, test_tag, test_word_len, test_char_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model-3/model-3.ckpt\n"
     ]
    }
   ],
   "source": [
    "tagger.restore('models/{}/{}.ckpt'.format(model_name, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9393e2b76b614908a3b048c493b60d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3453), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.722556657223796, 5648, 4081)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tagger.predict(test_data, word_to_index, char2index)\n",
    "accuracy(pd.Series(prediction).map(index_to_tag), pd.Series(test_data[2]).map(index_to_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237a6e6a8b4c455c98362a07a0107dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.9398798461338981, 23137, 21746)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tagger.predict(train_data, word_to_index, char2index)\n",
    "accuracy(pd.Series(prediction).map(index_to_tag), pd.Series(train_data[2]).map(index_to_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6379959455545902"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.equal(prediction, test_data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e90b3ff08a420aa7dc6f3c45a13651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3459), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.7869606598984772, 6304, 4961)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tagger.predict(val_data, word_to_index, char2index)\n",
    "accuracy(pd.Series(prediction).map(index_to_tag), pd.Series(val_data[2]).map(index_to_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7193696883852692, 5648, 4063)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [O, O, B-LOC, O, B-ORG, O, O, B-LOC, O, O, O, O]\n",
       "1                                       [B-PER, I-PER]\n",
       "2                   [B-ORG, O, B-ORG, I-ORG, I-ORG, O]\n",
       "3    [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...\n",
       "4    [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(prediction[:5]).map(index_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]\n",
       "1                                       [B-PER, I-PER]\n",
       "2                   [B-LOC, O, B-LOC, I-LOC, I-LOC, O]\n",
       "3    [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...\n",
       "4    [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_data[2][:5]).map(index_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 7)\n",
      "array([[['i', '#', '#', '#', '#', '#', '#'],\n",
      "        ['a', 'm', '#', '#', '#', '#', '#'],\n",
      "        ['n', 'o', 't', '#', '#', '#', '#'],\n",
      "        ['a', '#', '#', '#', '#', '#', '#'],\n",
      "        ['s', 't', 'u', 'd', 'e', 'n', 't']],\n",
      "\n",
      "       [['y', 'o', 'u', '#', '#', '#', '#'],\n",
      "        ['a', 'r', 'e', '#', '#', '#', '#'],\n",
      "        ['a', '#', '#', '#', '#', '#', '#'],\n",
      "        ['t', 'e', 'a', 'c', 'h', 'e', 'r'],\n",
      "        ['#', '#', '#', '#', '#', '#', '#']]], dtype='<U1')\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "             [['i', '#', '#', '#', '#', '#', '#'], \n",
    "              ['a', 'm', '#', '#', '#', '#', '#'],\n",
    "              ['n', 'o', 't', '#', '#', '#', '#'],\n",
    "              ['a', '#', '#', '#', '#', '#', '#'],\n",
    "              ['s', 't', 'u', 'd', 'e', 'n', 't']],\n",
    "              \n",
    "             [['y', 'o', 'u', '#', '#', '#', '#'], \n",
    "              ['a', 'r', 'e', '#', '#', '#', '#'],\n",
    "              ['a', '#', '#', '#', '#', '#', '#'],\n",
    "              ['t', 'e', 'a', 'c', 'h', 'e', 'r'],\n",
    "              ['#', '#', '#', '#', '#', '#', '#']]])\n",
    "print(x.shape)\n",
    "pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7)\n",
      "[['i' '#' '#' '#' '#' '#' '#']\n",
      " ['a' 'm' '#' '#' '#' '#' '#']\n",
      " ['n' 'o' 't' '#' '#' '#' '#']\n",
      " ['a' '#' '#' '#' '#' '#' '#']\n",
      " ['s' 't' 'u' 'd' 'e' 'n' 't']\n",
      " ['y' 'o' 'u' '#' '#' '#' '#']\n",
      " ['a' 'r' 'e' '#' '#' '#' '#']\n",
      " ['a' '#' '#' '#' '#' '#' '#']\n",
      " ['t' 'e' 'a' 'c' 'h' 'e' 'r']\n",
      " ['#' '#' '#' '#' '#' '#' '#']]\n"
     ]
    }
   ],
   "source": [
    "y = x.reshape((-1, 7))\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = set('aaaafdafkf,djfpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char.add('aaaafdafkf,djfpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [w for word in data_df['word'] for w in word ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = set(''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10, 1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 47, 9)\n",
      "(2, 47, 9)\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "batch_size = 2\n",
    "for batch_word, batch_char, batch_tag, batch_wlen, batch_clen in next_batch(train_data, batch_size, word2index, char2index):\n",
    "\n",
    "    print(batch_char.shape)  \n",
    "    for c in batch_char:\n",
    "        x.append(c)\n",
    "    break\n",
    "    \n",
    "print(np.array(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([list([[37, 53, 60, 60, 60, 60, 60, 60, 60], [50, 37, 42, 37, 35, 52, 51, 60, 60], [39, 37, 50, 45, 33, 46, 60, 60, 60], [35, 33, 44, 44, 60, 60, 60, 60, 60], [52, 47, 60, 60, 60, 60, 60, 60, 60], [34, 47, 57, 35, 47, 52, 52, 60, 60], [34, 50, 41, 52, 41, 51, 40, 60, 60], [44, 33, 45, 34, 60, 60, 60, 60, 60], [15, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60]]),\n",
      "       list([[48, 37, 52, 37, 50, 60, 60, 60, 60], [34, 44, 33, 35, 43, 34, 53, 50, 46], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60], [60, 60, 60, 60, 60, 60, 60, 60, 60]])],\n",
      "      dtype=object)\n"
     ]
    }
   ],
   "source": [
    "pprint(batch_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[37, 53, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [50, 37, 42, 37, 35, 52, 51, 60, 60],\n",
      "  [39, 37, 50, 45, 33, 46, 60, 60, 60],\n",
      "  [35, 33, 44, 44, 60, 60, 60, 60, 60],\n",
      "  [52, 47, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [34, 47, 57, 35, 47, 52, 52, 60, 60],\n",
      "  [34, 50, 41, 52, 41, 51, 40, 60, 60],\n",
      "  [44, 33, 45, 34, 60, 60, 60, 60, 60],\n",
      "  [15, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60]],\n",
      " [[48, 37, 52, 37, 50, 60, 60, 60, 60],\n",
      "  [34, 44, 33, 35, 43, 34, 53, 50, 46],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "  [60, 60, 60, 60, 60, 60, 60, 60, 60]]]\n"
     ]
    }
   ],
   "source": [
    "pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<DATE>'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[0-9]+[\\-|/][0-9]+[\\-|/][0-9]+', '<DATE>', '102-0-00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'[0-9]+[\\+|\\-|,|.|/]+', '0', '102-0-0-0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
